# Quantic Search: Proyecto RAG Multimodal

---

## üöÄ Resumen

**Quantic Search** es un sistema avanzado de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) que permite a los usuarios buscar informaci√≥n usando **texto, im√°genes, o una combinaci√≥n de ambos**. Impulsado por inteligencia artificial multimodal, este proyecto utiliza modelos de √∫ltima generaci√≥n para la comprensi√≥n de im√°genes, el procesamiento de texto y la IA generativa. El objetivo es ofrecer respuestas coherentes y ricas en contexto, bas√°ndose en la informaci√≥n visual recuperada.

La idea principal es mejorar la b√∫squeda tradicional basada solo en texto, incorporando se√±ales visuales para una interacci√≥n m√°s intuitiva y completa con la informaci√≥n.

---

## ‚ú® Caracter√≠sticas Principales

* **B√∫squeda Multimodal:** Consulta el sistema con descripciones de texto, subiendo im√°genes o con ambos al mismo tiempo.
* **Subtitulado de Im√°genes:** Genera autom√°ticamente descripciones textuales para las im√°genes cargadas utilizando el modelo **BLIP**.
* **Integraci√≥n con Base de Datos Vectorial:** Emplea **Pinecone** para almacenar y recuperar eficientemente las incrustaciones (embeddings) de im√°genes y texto.
* **Narrativa Contextual:** Utiliza **Gemini-1.5-Flash** de Google para crear respuestas narrativas basadas en el contexto visual recuperado.
* **Backend Escalable:** Desarrollado con **FastAPI** para una API robusta y de alto rendimiento.
* **Frontend Interactivo:** Una moderna aplicaci√≥n **React** que proporciona una experiencia de usuario fluida y con retroalimentaci√≥n en tiempo real.

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

### Backend y N√∫cleo RAG

* **Python:** Lenguaje de programaci√≥n principal.
* **FastAPI:** Framework web para construir la API.
* **PyTorch:** Framework de aprendizaje profundo para la inferencia de modelos.
* **Hugging Face Transformers:** Incluye los modelos **CLIP** (para incrustaciones de texto e imagen) y **BLIP** (para subt√≠tulos de im√°genes).
* **Pinecone:** Base de datos vectorial para la gesti√≥n eficiente de embeddings.
* **Google Gemini-1.5-Flash:** Modelo de lenguaje grande (LLM) para la generaci√≥n de texto.
* **Pillow (PIL):** Para procesamiento de im√°genes.
* **`python-dotenv`:** Para la gesti√≥n de variables de entorno.
* **CORS Middleware:** Para manejar las pol√≠ticas de seguridad de origen cruzado (CORS).

### Frontend

* **React:** Biblioteca JavaScript para construir la interfaz de usuario.
* **Tailwind CSS:** Para el dise√±o r√°pido y el estilizado de la UI.
* **TypeScript:** (Usado para mejorar la robustez y el mantenimiento del c√≥digo, aunque no se muestra expl√≠citamente en el fragmento de `App.js`).

---

## üìÇ Estructura del Proyecto

El proyecto est√° organizado en tres componentes principales:

1.  **Backend (`backend.py`):** Gestiona las solicitudes API con FastAPI, act√∫a como orquestador para el m√≥dulo RAG y maneja CORS y errores.
2.  **N√∫cleo RAG (`rag.py`):** Contiene toda la l√≥gica de IA/ML, inicializando y gestionando las conexiones a Pinecone y los modelos CLIP, BLIP y Gemini. Proporciona m√©todos para la descripci√≥n de im√°genes (`describir_img`), la recuperaci√≥n multimodal (`retrival`), la generaci√≥n de prompts (`generar_prompt_narrativo`), y la ejecuci√≥n general de la b√∫squeda (`search`).
3.  **Frontend (Aplicaci√≥n React):** Proporciona la interfaz de usuario para interactuar con el backend. Incluye componentes para el encabezado, el formulario de b√∫squeda, el carrusel de im√°genes, el panel de resultados, un spinner de carga y mensajes de error. Gestiona la entrada del usuario (texto/imagen), env√≠a solicitudes al backend y muestra los resultados.

---

## ‚öôÔ∏è Configuraci√≥n e Instalaci√≥n

### Prerrequisitos

* Python 3.8+
* Node.js y npm (para el frontend)
* Acceso a una clave API de Pinecone
* Acceso a una clave API de Google Gemini

### 1. Clonar el repositorio

```bash
git clone <tu-url-del-repositorio>
cd quantic-search
```

### 2. Configuraci√≥n del Backend
#### 1. Crear un entorno virtual
#### 2. Instala las dependencias
#### 3. Confuga tus claves API

### 3. Configuraci√≥n del Frontend
#### 1. Navega al directorio frontend
#### 2. Instala las dependencias de Node.js: npm intsall
#### 3. 3.	Inicia el servidor de desarrollo de React: npm start

### 4. Ejecutar el backend
Desde la ra√≠z del proyecto (despu√©s de activar tu entorno virtual):
#### 1.	Ejecuta el servidor FastAPI: uvicorn backend:app --host 0.0.0.0 --port 8000 --reload
#### 2.	El backend estar√° accesible en http://localhost:8000.

## üöÄ Uso

1. Aseg√∫rate de que tanto el backend (FastAPI) como el frontend (React) est√©n ejecut√°ndose.  
2. Abre tu navegador y navega a la URL del frontend (normalmente [http://localhost:3000](http://localhost:3000)).  
3. Usa la barra de b√∫squeda para:  
   - Escribir una consulta de texto.  
   - Subir una imagen.  
   - Hacer ambas para realizar una b√∫squeda multimodal.  
4. El panel de resultados mostrar√° la respuesta narrativa generada por la IA y un carrusel de im√°genes relevantes recuperadas de la base de datos.

---

## üí° C√≥mo Funciona (Flujo Conceptual)

1. **Entrada del Usuario:** El frontend captura una consulta de texto y/o una imagen cargada.  
2. **Llamada a la API:** El frontend env√≠a estos datos al endpoint `/search` de FastAPI.  
3. **Procesamiento del Backend:**  
   - Si se proporciona una imagen, el modelo BLIP primero genera una descripci√≥n textual (subt√≠tulo) de la imagen.  
   - La consulta de texto (original o generada a partir de la imagen) es luego procesada por el modelo CLIP para crear una incrustaci√≥n vectorial de alta dimensi√≥n.  
   - Esta incrustaci√≥n se utiliza para consultar la base de datos vectorial Pinecone y recuperar los subt√≠tulos de im√°genes m√°s sem√°nticamente similares y sus URLs asociadas.  
4. **Generaci√≥n Contextual:**  
   - Los subt√≠tulos recuperados forman el "contexto" para el Modelo de Lenguaje Grande.  
   - Se construye un prompt sofisticado, combinando la consulta original del usuario (o la descripci√≥n de la imagen) con el contexto recuperado.  
   - Este prompt se env√≠a a Google Gemini-1.5-Flash.  
5. **Respuesta:** Gemini genera una narrativa o explicaci√≥n coherente basada en el contexto proporcionado.  
6. **Mostrar Resultados:** El backend env√≠a la narrativa generada y los detalles de las im√°genes recuperadas de vuelta al frontend, que luego los muestra al usuario.

---

## üìÑ Licencia

Este proyecto es de c√≥digo abierto. Consulta el archivo LICENSE para m√°s detalles.

---

## üìû Contacto

Para cualquier pregunta o colaboraci√≥n, ¬°no dudes en contactarnos!
